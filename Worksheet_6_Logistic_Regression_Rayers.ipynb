{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rayers-Ranjitkar/Rayers_AI_Worksheets/blob/main/Worksheet_6_Logistic_Regression_Rayers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFKoS2dWFqTg"
      },
      "source": [
        "**# Classification-Introduction:**\n",
        "\n",
        "In Statistics and Machine Learning, classification is a type of supervised learning task, where training data with known class labels are given and are used to estimate a function, which can map a unseen  features (Idependent  Variables $X$) to its class label (Dependent Variable $Y$) .\n",
        "\n",
        "Tasks in Supervised Learning-Classification Task can be:\n",
        "  *   Binary Classification.\n",
        "  *   Multiclass Classification.\n",
        "\n",
        "This Notebook consist of One Sections:\n",
        "  *   Section-1: This section will discuss how can we use Logistic Regression to perform Binary Classification Task.\n",
        "    *   We also describe various helper function such as Sigmoid Function, Cost fucntion and disccuss.\n",
        "\n",
        "Author-Siman Giri."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSBFhL05IdaO"
      },
      "source": [
        "# Section-1: Binary Classification with Logistic Regression.\n",
        "\n",
        "___\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exqiG--qJKJh"
      },
      "source": [
        "## Helper Function:\n",
        "\n",
        "Let's discuss two function we need to further implement logistic regression.\n",
        "  1.   Sigmoid Function.\n",
        "  2.   Cost(Loss) Function.\n",
        "\n",
        "___\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sKX3VdDJsOk"
      },
      "source": [
        "### Sigmoid Function:\n",
        "A function  $[g : R -> R] $  is said to be a sigmoid function, if the function is bounded, differentiable, real function that is defined for all real input values and has a non-negative derivative at each point and exactly on inflection point. The sigmoid fucntion has a characterstic \"S\" shaped curved also known as sigmoid curve.  [--- Wikipedia]\n",
        "\n",
        "A comon example of a sigmoid function is the logistic function described as below:\n",
        "$$ g(x) = \\frac{1}{1+e^{-x}}, $$\n",
        "\n",
        "for $x \\in \\mathbb{R}$.\n",
        "\n",
        "The next two code blocks construct and plot this function.\n",
        "___"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATSH37zsE0LX"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zOXBvw9DNuyh"
      },
      "outputs": [],
      "source": [
        "def logistic_function(x):\n",
        "  \"\"\"\n",
        "  Computes the logistic function applied to any value of x.\n",
        "  Arguments:\n",
        "    x: scalar or numpy array of any size.\n",
        "  Returns:\n",
        "    y: logistic function applied to x.\n",
        "  \"\"\"\n",
        "  import numpy as np\n",
        "  # Your Code here\n",
        "  #z becomes x, which we pass from below\n",
        "  y = 1/ (1+ np.exp(-x))\n",
        "  return y\n",
        "\n",
        "\n",
        "# Test Function:\n",
        "# For scalar:\n",
        "x = 0\n",
        "y = logistic_function(x)\n",
        "print(f\"logistic({x}) = {logistic_function(x)}\")\n",
        "# For Array:\n",
        "x_arr = np.array([-3, -1, 1, 3])\n",
        "y_arr = logistic_function(x_arr)\n",
        "print(f\"logistic({x_arr}) = {logistic_function(x_arr)}\") #numpy -> vectorized operations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_GyQ3WHQPbJ"
      },
      "outputs": [],
      "source": [
        "# Plotting the sigmoid function:\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "plt.figure(figsize = (10, 5))\n",
        "x = np.linspace(-11, 11, 100)\n",
        "plt.plot(x, logistic_function(x), color = 'red')\n",
        "plt.xlabel(\"x\", fontsize = 14)\n",
        "plt.ylabel(\"g(x)\", fontsize = 14)\n",
        "plt.title(\"Standard logistic function\", fontsize = 14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the logistic (sigmoid) function\n",
        "def logistic_function(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Define test function\n",
        "def test_logistic_function():\n",
        "    \"\"\"\n",
        "    Test cases for the logistic_function.\n",
        "    \"\"\"\n",
        "\n",
        "    # Test with scalar input\n",
        "    x_scalar = 0\n",
        "    expected_output_scalar = round(1 / (1 + np.exp(-0)), 3)  # Expected output: 0.5\n",
        "    assert round(logistic_function(x_scalar), 3) == expected_output_scalar, \"Test failed for scalar input\"\n",
        "\n",
        "    # Test with positive scalar input\n",
        "    x_pos = 2\n",
        "    expected_output_pos = round(1 / (1 + np.exp(-2)), 3)  # Expected output: ~0.881\n",
        "    assert round(logistic_function(x_pos), 3) == expected_output_pos, \"Test failed for positive scalar input\"\n",
        "\n",
        "    # Test with negative scalar input\n",
        "    x_neg = -3\n",
        "    expected_output_neg = round(1 / (1 + np.exp(3)), 3)  # Expected output: ~0.047\n",
        "    assert round(logistic_function(x_neg), 3) == expected_output_neg, \"Test failed for negative scalar input\"\n",
        "\n",
        "    # Test with numpy array input\n",
        "    x_array = np.array([0, 2, -3])\n",
        "    expected_output_array = np.array([0.5, 0.881, 0.047])  # Adjusted expected values rounded to 3 decimals\n",
        "    # Use np.round to round the array element-wise and compare\n",
        "    assert np.all(np.round(logistic_function(x_array), 3) == expected_output_array), \"Test failed for numpy array input\"\n",
        "\n",
        "    print(\"All tests passed!\")\n",
        "\n",
        "# Run the test case\n",
        "test_logistic_function()\n"
      ],
      "metadata": {
        "id": "caOEA0n8Ao-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6QM_2RWQsTw"
      },
      "source": [
        "### Cost(Loss) Function:\n",
        "Cost vs Loss Function:\n",
        "\n",
        "In general, loss function corresponds to observed error value between target value and predicted value for single observation/data points whereas cost function represents the average loss over a group of observations/data points.\n",
        "\n",
        "Due to the non-linearity introduced by logistic function in the function, we can not use squared loss as an loss function, instead we make use of log-loss function, given by:\n",
        "\n",
        "$$ L(y, y') = -y \\log\\left(y'\\right) - \\left(1 - y\\right) \\log\\left(1 - y'\\right), $$\n",
        "\n",
        "Where:\n",
        "*   $y$ : True target value (taking values $0$ or $1$)\n",
        "*   $y'$ : Predeicted target value ( predicted probability of $y$ being $1$ and vice versa.)\n",
        "\n",
        "The basis intution behind the log-loss function is, the loss value should be minimum when our predicted probability values are closer to true target value.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDLxGVmwUQQ7"
      },
      "outputs": [],
      "source": [
        "def log_loss(y_true, y_pred):\n",
        "  \"\"\"\n",
        "  Computes log loss for true target value y ={0 or 1} and predicted target value y' inbetween {0-1}.\n",
        "  Arguments:\n",
        "    y_true (scalar): true target value {0 or 1}.\n",
        "    y_pred (scalar): predicted taget value {0-1}.\n",
        "  Returns:\n",
        "    loss (float): loss/error value\n",
        "  \"\"\"\n",
        "  import numpy as np\n",
        "  #loss = -y * np.log(y_pred) - (1-y_true) * np.log(1-y_pred)\n",
        "  loss = (-y_true * np.log(y_pred)) - ((1-y_true) * np.log(1-y_pred))\n",
        "  return loss\n",
        "\n",
        "# Test function:\n",
        "y_true, y_pred = 0, 0.1\n",
        "print(f'log loss({y_true}, {y_pred}) ==> {log_loss(y_true, y_pred)}')\n",
        "print(\"+++++++++++++--------------------------++++++++++++++++++++++++\")\n",
        "y_true, y_pred = 1, 0.9\n",
        "print(f'log loss({y_true}, {y_pred}) ==> {log_loss(y_true, y_pred)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOWpQJFK30gr"
      },
      "outputs": [],
      "source": [
        "# Plot the loss Function:\n",
        "import matplotlib.pyplot as plt\n",
        "fig, ax = plt.subplots(1, 2, figsize = (15, 6), sharex = True, sharey = True)\n",
        "y_pred = np.linspace(0.0001, 0.9999, 100)\n",
        "ax[0].plot(y_pred, log_loss(0, y_pred), color = 'red')\n",
        "ax[0].set_title(\"y = 0\", fontsize = 14)\n",
        "ax[0].set_xlabel(\"y_pred\", fontsize = 14)\n",
        "ax[0].set_ylabel(\"log loss\", fontsize = 14)\n",
        "ax[1].plot(y_pred, log_loss(1, y_pred), color = 'red')\n",
        "ax[1].set_title(\"y = 1\", fontsize = 14)\n",
        "ax[1].set_xlabel(\"y_pred\", fontsize = 14)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKRbJzqS4W9D"
      },
      "source": [
        "### Cost Function:\n",
        "As described above, we determine the cost function as an average of loss function value calculated for each observation/datapoints.\n",
        "\n",
        "Let $y = (y_1,....,y_n)$ be the true target values{(0 or 1)}  and $y'=(y_1',....y_n')$ be the corresponding predicted target values in between {(0-1)}, then the cost function be:\n",
        "\n",
        "$$ C(\\mathbf{y}, \\mathbf{y'}) = \\frac{1}{n}\\sum_{i = 1}^n L(y_i, y_i').\\tag{0} $$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "938aG1sz95b6"
      },
      "outputs": [],
      "source": [
        "# Cost function - using vectorization\n",
        "def cost_function(y_true, y_pred):\n",
        "    #each_loss = 0 #self\n",
        "    \"\"\"\n",
        "    Computes log loss for inputs true value (0 or 1) and predicted value (between 0 and 1)\n",
        "    Args:\n",
        "      y_true    (array_like, shape (m,)): array of true values (0 or 1)\n",
        "      y_pred (array_like, shape (m,)): array of predicted values (probability of y_pred being 1)\n",
        "    Returns:\n",
        "      cost (float): nonnegative cost corresponding to y_true and y_pred\n",
        "    \"\"\"\n",
        "    assert len(y_true) == len(y_pred), \"Length of true values and length of predicted values do not match\"\n",
        "    n = len(y_true)\n",
        "\n",
        "    for i in range(n): #index starts from 0\n",
        "      #loss_vec = y_true[i] - y_pred[i] #self\n",
        "      #each_loss = each_loss + loss_vec #self\n",
        "      loss_vec = [log_loss(y_true[i],y_pred[i]) for i in range(n)] #List comprehension: after each returns it appends returned thing to a list, and returns entire list at last\n",
        "      cost = 1/n * np.sum(loss_vec) #np.mean(loss_vec)\n",
        "      return cost\n",
        "\n",
        "    #cost = each_loss / n #self\n",
        "    #return cost #self\n",
        "\n",
        "y_true, y_pred = np.array([0, 1, 0]), np.array([0.4, 0.6, 0.25])\n",
        "print(f\"cost_function({y_true}, {y_pred}) = {cost_function(y_true, y_pred)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--SYJgFONMZj"
      },
      "source": [
        "Extending the cost function for logistic regression to be used with model parameters.\n",
        "\n",
        "Function we are estimating:\n",
        "\n",
        "$$ y' = g\\left(\\mathbf{x} \\cdot \\mathbf{Î¸} + b\\right) = \\frac{1}{1 + e^{-\\left(\\mathbf{x} \\cdot \\mathbf{\\theta} + b\\right)}}. \\tag{1} $$\n",
        "\n",
        "Where:\n",
        "- $\\theta$ parameters (Coefficinet of feature variable) also known as weights.\n",
        "-$b$ parameters ( intercept of the function) also known as bias.\n",
        "\n",
        "Assume;\n",
        "- X: Matirix of n independent variables;\n",
        "- Y: Matrix of n dependent variables;\n",
        "- Y': Matrix of n predicted variables; and are represented as follows:\n",
        "\n",
        "$$ \\mathbf{X} = \\begin{pmatrix}\n",
        "x_{1,1} & x_{1,2} & \\cdots & x_{1,n} \\newline\n",
        "x_{2,1} & x_{2,2} & \\cdots & x_{2,n} \\newline\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\newline\n",
        "x_{m,1} & x_{m,2} & \\cdots & x_{m,n}\n",
        "\\end{pmatrix},\\;\\;\\;\\;\n",
        "\\mathbf{y} = \\begin{pmatrix}\n",
        "y_1 \\newline\n",
        "y_2 \\newline\n",
        "\\vdots \\newline\n",
        "y_m\n",
        "\\end{pmatrix},\\;\\;\\;\\;\n",
        "\\mathbf{y'} = \\begin{pmatrix}\n",
        "g\\left(\\mathbf{x_1} \\cdot \\mathbf{\\theta} + b\\right) \\newline\n",
        "g\\left(\\mathbf{x_2} \\cdot \\mathbf{\\theta} + b\\right) \\newline\n",
        "\\vdots \\newline\n",
        "g\\left(\\mathbf{x_n} \\cdot \\mathbf{\\theta} + b\\right)\n",
        "\\end{pmatrix}. \\tag{2} $$\n",
        "\n",
        "Now rewrite cost funtion defined in $(0)$ for model parameters as:\n",
        "\n",
        "$$ J\\left(\\mathbf{w}, b\\right) := C\\left(\\mathbf{y}, \\mathbf{y'} \\,\\vert\\, \\mathbf{X}, \\mathbf{w}, b \\right) = \\frac{1}{m}\\sum_{i = 1}^m L\\left(y_i, \\frac{1}{1 + e^{-\\left(\\mathbf{x_i} \\cdot \\mathbf{w} + b\\right)}}\\right) $$\n",
        "$$= \\frac{1}{m}\\sum_{i = 1}^m \\left[ -y_i \\log\\left(\\frac{1}{1 + e^{-\\left(\\mathbf{x_i} \\cdot \\mathbf{w} + b\\right)}}\\right) - \\left(1 - y_i\\right) \\log\\left(1 - \\frac{1}{1 + e^{-\\left(\\mathbf{x_i} \\cdot \\mathbf{w} + b\\right)}}\\right) \\right]. \\tag{3} $$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5k8Pi5r5ipE"
      },
      "outputs": [],
      "source": [
        "# Function to compute cost function in terms of model parameters - using vectorization\n",
        "def costfunction_logreg(X, y, w, b):\n",
        "    \"\"\"\n",
        "    Computes the cost function, given data and model parameters\n",
        "    Args:\n",
        "      X (ndarray, shape (m,n))  : data on features, m observations with n features\n",
        "      y (array_like, shape (m,)): array of true values of target (0 or 1)\n",
        "      w (array_like, shape (n,)): weight parameters of the model\n",
        "      b (float)                 : bias parameter of the model\n",
        "    Returns:\n",
        "      cost (float): nonnegative cost corresponding to y and y_dash\n",
        "    \"\"\"\n",
        "    m, n = X.shape\n",
        "    assert len(y) == m, \"Number of feature observations and number of target observations do not match\"\n",
        "    assert len(w) == n, \"Number of features and number of weight parameters do not match\"\n",
        "    z = b + np.dot(X,w)\n",
        "    y_pred = logistic_function(z)\n",
        "    cost = cost_function(y,y_pred)\n",
        "    return cost\n",
        "\n",
        "X, y, w, b = np.array([[10, 20], [-10, 10]]), np.array([1, 0]), np.array([0.5, 1.5]), 1\n",
        "print(f\"cost for logistic regression(X = {X}, y = {y}, w = {w}, b = {b}) = {costfunction_logreg(X, y, w, b)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAGw3-xqXJkW"
      },
      "source": [
        "### Gradient Descent Algorithm\n",
        "\n",
        "The gradient descent algorithm is a first order iterative optimization algorithm for finding alocal minimum of a differentiable function.\n",
        "\n",
        "The Algorithm:\n",
        "$$ \\begin{align*}\n",
        "& \\text{repeat until convergence:}\\; \\{ \\newline\n",
        "& w_j := w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j},\\; \\text{ for } j = 1, 2, \\ldots, n; \\newline\n",
        "& b := b -  \\alpha \\frac{\\partial J(\\mathbf{w}, b)}{\\partial b}. \\tag{4} \\newline\n",
        "& \\}\n",
        "\\end{align*} $$\n",
        "\n",
        "- where $\\alpha$ is the learning rate, and the parameters $\\mathbf{w} = (w_1, w_2, \\cdots, w_n)$ and $b$ are updated simultaniously in each iteration.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7z9-J9sKX0Sd"
      },
      "source": [
        "**Compute Gradient:**\n",
        "\n",
        "Before we can implement the gradient descent algorithm, we need to compute the gradients, which are the partial derivatives of cost function $J(w,b)$ and are as follows:\n",
        "\n",
        "\n",
        "$$ \\begin{align*}\n",
        "& \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} = \\frac{1}{m} \\sum\\limits_{i = 1}^m \\left(\\frac{1}{1 + e^{-\\left(\\mathbf{x_i} \\cdot \\mathbf{w} + b\\right)}} - y_i\\right) x_{i,j},\\;\\; \\text{ for } j = 1, 2, \\ldots, n; \\newline\n",
        "& \\frac{\\partial J(\\mathbf{w}, b)}{\\partial b} = \\frac{1}{m} \\sum\\limits_{i = 1}^m \\left(\\frac{1}{1 + e^{-\\left(\\mathbf{x_i} \\cdot \\mathbf{w} + b\\right)}} - y_i\\right).\n",
        "\\end{align*} $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cH6F9uAhYWlL"
      },
      "outputs": [],
      "source": [
        "# Function to compute gradients of the cost function with respect to model parameters - using vectorization\n",
        "def compute_gradient(X, y, w, b):\n",
        "    \"\"\"\n",
        "    Computes gradients of the cost function with respect to model parameters\n",
        "    Args:\n",
        "      X (ndarray, shape (m,n))  : data on features, m observations with n features\n",
        "      y (array_like, shape (m,)): array of true values of target (0 or 1)\n",
        "      w (array_like, shape (n,)): weight parameters of the model\n",
        "      b (float)                 : bias parameter of the model\n",
        "    Returns:\n",
        "      grad_w (array_like, shape (n,)): gradients of the cost function with respect to the weight parameters\n",
        "      grad_b (float)                 : gradient of the cost function with respect to the bias parameter\n",
        "    \"\"\"\n",
        "    m, n = X.shape\n",
        "    assert len(y) == m, \"Number of feature observations and number of target observations do not match\"\n",
        "    assert len(w) == n, \"Number of features and number of weight parameters do not match\"\n",
        "\n",
        "    z = b + np.dot(X,w)\n",
        "    y_pred = logistic_function(z)\n",
        "    grad_w =  np.dot((y_pred - y),X) /m\n",
        "    grad_b =  np.sum(y_pred - y) /m\n",
        "    return grad_w, grad_b\n",
        "\n",
        "X, y, w, b = np.array([[10, 20], [-10, 10]]), np.array([1, 0]), np.array([0.5, 1.5]), 1\n",
        "print(f\"gradient of logistic regression parameters (X = {X}, y = {y}, w = {w}, b = {b}) = {compute_gradient(X, y, w, b)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1KuJlZQYxJU"
      },
      "source": [
        "**Implement Gradient Descent for Logistic Regression.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hd8IKGNIY4Dj"
      },
      "outputs": [],
      "source": [
        "# Gradient descent algorithm for logistic regression\n",
        "def gradient_descent(X, y, w, b, alpha, n_iter, show_cost = True, show_params = False):\n",
        "    \"\"\"\n",
        "    Implements batch gradient descent algorithm to learn and update model parameters\n",
        "    with prespecified number of interations and learning rate\n",
        "    Args:\n",
        "      X (ndarray, shape (m,n))  : data on features, m observations with n features\n",
        "      y (array_like, shape (m,)): true values of target (0 or 1)\n",
        "      w (array_like, shape (n,)): initial value of weight parameters\n",
        "      b (scalar)                : initial value of bias parameter\n",
        "      cost_func                 : function to compute cost\n",
        "      grad_func                 : function to compute gradients of cost with respect to model parameters\n",
        "      alpha (float)             : learning rate\n",
        "      n_iter (int)              : number of iterations\n",
        "    Returns:\n",
        "      w (array_like, shape (n,)): updated values of weight parameters\n",
        "      b (scalar)                : updated value of bias parameter\n",
        "    \"\"\"\n",
        "    from tqdm.contrib import itertools\n",
        "    import math\n",
        "    import tqdm\n",
        "    from time import sleep\n",
        "    m, n = X.shape\n",
        "    assert len(y) == m, \"Number of feature observations and number of target observations do not match\"\n",
        "    assert len(w) == n, \"Number of features and number of weight parameters do not match\"\n",
        "    cost_history, params_history = [], []\n",
        "    for i, j in itertools.product(range(n_iter), range(1)):\n",
        "        grad_w, grad_b = compute_gradient(X, y, w, b)\n",
        "        w += -alpha * grad_w\n",
        "        b += -alpha * grad_b\n",
        "        cost = costfunction_logreg(X,y,w,b)\n",
        "        cost_history.append(cost)\n",
        "        params_history.append([w, b])\n",
        "\n",
        "\n",
        "    return w, b, cost_history, params_history\n",
        "\n",
        "X, y, w, b, alpha, n_iter = np.array([[0.1, 0.2], [-0.1, 0.1]]), np.array([1, 0]), np.array([0., 0.]), 0., 0.1, 100000\n",
        "w_out, b_out, cost_history, params_history = gradient_descent(X, y, w, b, alpha, n_iter, show_cost = False, show_params = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bI96yman53sw"
      },
      "outputs": [],
      "source": [
        "# Plotting cost over iteration\n",
        "plt.figure(figsize = (9, 6))\n",
        "plt.plot(cost_history)\n",
        "plt.xlabel(\"Iteration\", fontsize = 14)\n",
        "plt.ylabel(\"Cost\", fontsize = 14)\n",
        "plt.title(\"Cost vs Iteration\", fontsize = 14)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXOAgvPxabDh"
      },
      "source": [
        "# Logistic Regression-Scratch Implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_aT7vJFaPwB"
      },
      "outputs": [],
      "source": [
        "path_dataset = \"/content/drive/MyDrive/Dataset/Iris.csv\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "onyMS1nmWWq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5CdYY2dYuIcr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwYRQb82vgh4"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(path_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "USgZSg4KvkIo"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLM-ywy8vl7W"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_9e-pvYvoOG"
      },
      "outputs": [],
      "source": [
        "data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XbakzEvA1HkS"
      },
      "outputs": [],
      "source": [
        "data = data.drop([\"Id\"], axis=1)\n",
        "data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qkXqaIfWvq3m"
      },
      "outputs": [],
      "source": [
        "data.Species = data.Species.replace(data.Species.unique(),[*range(len(data.Species.unique()))])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ze1dC6Mqw-V7"
      },
      "outputs": [],
      "source": [
        "binary_iris = data[data.Species!=2].reset_index(drop=True)\n",
        "binary_iris"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7ZkU9oTxEZJ"
      },
      "outputs": [],
      "source": [
        "X = binary_iris.iloc[:, :-1].values\n",
        "Y = binary_iris.iloc[:,-1].values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBolOOAg00UZ"
      },
      "outputs": [],
      "source": [
        "# Sanity Check:\n",
        "if X.shape[0] == Y.shape[0]:\n",
        "  print(\"Progress Further\")\n",
        "else:\n",
        "  print(\"X and Y are not created correctly\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3l2CA0F1u_y"
      },
      "source": [
        "# Train Test Split:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-yPeV5Nu01lX"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-8WO_TX1XpJ"
      },
      "outputs": [],
      "source": [
        "# Sanity Check:\n",
        "if x_train.shape[0] == y_train.shape[0]:\n",
        "  print(\"Progress Further\")\n",
        "else:\n",
        "  print(\"x_train and y_train are not created correctly\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWC1GJTG6KGg"
      },
      "source": [
        "# Model Fitting:\n",
        "\n",
        "Training of the Model:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HDA8HhY82CPX"
      },
      "outputs": [],
      "source": [
        "# Initiialization:\n",
        "w_init = np.zeros(x_train.shape[1])\n",
        "b_init = -1\n",
        "# Learning model parameters using gradient descent algorithm\n",
        "w_out, b_out, cost_history, params_history = gradient_descent(x_train,\n",
        "                                                       y_train,\n",
        "                                                       w = w_init,\n",
        "                                                       b = b_init,\n",
        "                                                       alpha = 0.1,\n",
        "                                                       n_iter = 2000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FnFlAjjI7B26"
      },
      "outputs": [],
      "source": [
        "# Plotting cost over iteration\n",
        "plt.figure(figsize = (9, 6))\n",
        "plt.plot(cost_history)\n",
        "plt.xlabel(\"Iteration\", fontsize = 14)\n",
        "plt.ylabel(\"Cost\", fontsize = 14)\n",
        "plt.title(\"Cost vs Iteration\", fontsize = 14)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Nt2tq4OIfRv"
      },
      "source": [
        "### **Prediction and Decision Boundry**\n",
        "- Prediction:\n",
        "\n",
        "  - Utilizing our trained weights and biases we first find the probabilities values y_probab for the x_test.\n",
        "Then, Y_probabiliy value is transformed to discrete class value using Decision boubdry.\n",
        "___\n",
        "- Decision Boundry:\n",
        "  - We first calculate the y_prediction as probabilities value, which is then converted to discrete class by using a threshold value. For instance, we take the threshold to be 0.5,then we classify the observation to class 1 and to class 0 otherwise.\n",
        "___\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVJoulDp7Ppt"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "def prediction(x_test, w_out, b_out):\n",
        "  \"\"\"\n",
        "  Computes the prediction for given test values.\n",
        "  Arguments: Inputs\n",
        "  x_test (nd array): Array of Test Independent variables.\n",
        "  w_out (nd array): Array of weights learned via gradient descent.\n",
        "  b_out (nd array) Array of bias learned via gradiebt descent.\n",
        "  Arguments: Output\n",
        "  y_pred (nd arrray): Array of Predicted dependent Variables.\n",
        "  \"\"\"\n",
        "  y_test_prob = logistic_function(np.matmul(x_test, w_out) + (b_out * np.ones(x_test.shape[0])))\n",
        "  y_pred = []\n",
        "  # Your code here\n",
        "  for p in y_test_prob:\n",
        "    if p >= 0.5:\n",
        "      y_pred.append(1)\n",
        "    else:\n",
        "      y_pred.append(0)\n",
        "\n",
        "  return y_pred\n",
        "# y_pred = (y_test_prob > 0.5).astype(int) #Instead of a loop we can write this too\n",
        "\n",
        "y_pred = prediction(x_test, w_out, b_out)\n",
        "y_pred = np.array(y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XV9A8zUSIPtG"
      },
      "outputs": [],
      "source": [
        "y_pred.shape == y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ItzfMfRqIVNO"
      },
      "outputs": [],
      "source": [
        "y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U94YcuS5IXea"
      },
      "outputs": [],
      "source": [
        "y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uy_j54t7IN8R"
      },
      "outputs": [],
      "source": [
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 8))\n",
        "ax.imshow(cm)\n",
        "ax.grid(False)\n",
        "ax.xaxis.set(ticks=(0, 1), ticklabels=('Predicted 0s', 'Predicted 1s'))\n",
        "ax.yaxis.set(ticks=(0, 1), ticklabels=('Actual 0s', 'Actual 1s'))\n",
        "ax.set_ylim(1.5, -0.5)\n",
        "for i in range(2):\n",
        "    for j in range(2):\n",
        "        ax.text(j, i, cm[i, j], ha='center', va='center', color='white')\n",
        "plt.show() #see false positive and false -ve is 0 meaning it didn't predict anything wrong = accuracy = 100%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fs5VFUEhFnEM"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Home-Work {Graded Task - Deadline - Next-workshop}\n",
        "With the help of Sklearn Library Implement a Multiclass Classification for any data of your choices.\n",
        "\n"
      ],
      "metadata": {
        "id": "EwoCzlwc2l39"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "iris_df = pd.read_csv(path_dataset);\n",
        "\n",
        "iris_df.head()"
      ],
      "metadata": {
        "id": "1kTxqpI92oWl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "43474770-8faf-4ac4-ce3f-a5fdad4fa629"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm      Species\n",
              "0   1            5.1           3.5            1.4           0.2  Iris-setosa\n",
              "1   2            4.9           3.0            1.4           0.2  Iris-setosa\n",
              "2   3            4.7           3.2            1.3           0.2  Iris-setosa\n",
              "3   4            4.6           3.1            1.5           0.2  Iris-setosa\n",
              "4   5            5.0           3.6            1.4           0.2  Iris-setosa"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2f2c392d-2488-4324-b10a-df53bf24994c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>SepalLengthCm</th>\n",
              "      <th>SepalWidthCm</th>\n",
              "      <th>PetalLengthCm</th>\n",
              "      <th>PetalWidthCm</th>\n",
              "      <th>Species</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>4.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>4.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>5.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2f2c392d-2488-4324-b10a-df53bf24994c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2f2c392d-2488-4324-b10a-df53bf24994c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2f2c392d-2488-4324-b10a-df53bf24994c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-a97e3f89-07b5-4419-8fb4-20295b5b7bad\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a97e3f89-07b5-4419-8fb4-20295b5b7bad')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-a97e3f89-07b5-4419-8fb4-20295b5b7bad button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "iris_df",
              "summary": "{\n  \"name\": \"iris_df\",\n  \"rows\": 150,\n  \"fields\": [\n    {\n      \"column\": \"Id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 43,\n        \"min\": 1,\n        \"max\": 150,\n        \"num_unique_values\": 150,\n        \"samples\": [\n          74,\n          19,\n          119\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SepalLengthCm\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.8280661279778629,\n        \"min\": 4.3,\n        \"max\": 7.9,\n        \"num_unique_values\": 35,\n        \"samples\": [\n          6.2,\n          4.5,\n          5.6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SepalWidthCm\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.4335943113621737,\n        \"min\": 2.0,\n        \"max\": 4.4,\n        \"num_unique_values\": 23,\n        \"samples\": [\n          2.3,\n          4.0,\n          3.5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"PetalLengthCm\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.7644204199522617,\n        \"min\": 1.0,\n        \"max\": 6.9,\n        \"num_unique_values\": 43,\n        \"samples\": [\n          6.7,\n          3.8,\n          3.7\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"PetalWidthCm\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.7631607417008414,\n        \"min\": 0.1,\n        \"max\": 2.5,\n        \"num_unique_values\": 22,\n        \"samples\": [\n          0.2,\n          1.2,\n          1.3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Species\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Iris-setosa\",\n          \"Iris-versicolor\",\n          \"Iris-virginica\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Separating columns and labels\n",
        "\n",
        "X = iris_df.drop(columns=[\"Id\", \"Species\"] );\n",
        "y = iris_df[\"Species\"]\n",
        "\n",
        "# x.head()\n",
        "# y.head()\n",
        "\n",
        "#Converting labels to numbers i.e. setosa = 0, veriscolor = 1 and iris-virginica = 2\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "encoder = LabelEncoder()  #It looks at unique classes in y, Assigns numbers automatically, and returns a numeric array # Creates an encoder object # LabelEncoder converts text labels into numbers\n",
        "y_encoded = encoder.fit_transform(y)\n",
        "\n",
        "print(y_encoded)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmS2G67P2JX8",
        "outputId": "42b82c30-a043-48fb-8837-e9bdb05f506a"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Performing 80/20 train-test split\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61a2bu5R3gdu",
        "outputId": "dc59a297-94a0-414e-8867-aceb441037d4"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(120, 4)\n",
            "(30, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#importing a classification algo to create the model\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model = LogisticRegression(max_iter=200) #defining no of iterations\n",
        "\n",
        "#fitting the model to our test_data_points :\n",
        "\n",
        "model.fit(X_train, y_train) #learns parameters like (w0, w1, w2, w3â¦) for linear regression + mean and sd deviation for transformers i.e. z = (mean - meu)/sd_deviation i.e. standard scaling\n",
        "\n",
        "#Predicting on the test data\n",
        "y_pred_train = model.predict(X_train)\n",
        "\n",
        "#Evaluating the accuracy on train dataset\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "accuracy = accuracy_score(y_train, y_pred_train)\n",
        "print(\"Accuracy on train dataset:\", accuracy)\n",
        "\n",
        "\n",
        "# Evaluating the accuracy on test dataset\n",
        "y_pred_test = model.predict(X_test)\n",
        "\n",
        "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
        "print(\"Accuracy on test dataset:\", test_accuracy)\n",
        "\n",
        "print(\"-\" * 50)\n",
        "print(\"Other classification Reports:\")\n",
        "from sklearn.metrics import classification_report # classification_report gives precision, recall, f1-score\n",
        "print(classification_report(y_test, y_pred_test))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpeLAq5W4CP1",
        "outputId": "790e8664-a557-418c-c8b3-57f3fbd4277d"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on train dataset: 0.975\n",
            "Accuracy on test dataset: 1.0\n",
            "--------------------------------------------------\n",
            "Other classification Reports:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       1.00      1.00      1.00         9\n",
            "           2       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Test Case for the prediction\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "new_flower = np.array([[5.1, 3.5, 1.4, 0.2]]) #specifying a 2d array of size (1x4) , np.array([5.1, 3.5, 1.4, 0.2]) -> size = (4,)\n",
        "\n",
        "# Predicting class\n",
        "prediction = model.predict(new_flower)\n",
        "\n",
        "print(\"Prediction species: \", prediction)\n",
        "\n",
        "# Converting number back to species name using encoder.inverse_transform\n",
        "predicted_species = encoder.inverse_transform(prediction)\n",
        "\n",
        "print(\"Predicted Species:\", predicted_species[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSHIJ21P-EKa",
        "outputId": "97a03d03-2893-45a6-8270-7496ec872d57"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction species:  [0]\n",
            "Predicted Species: Iris-setosa\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "generative_ai_disabled": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}